{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 20:41:41 WARN Utils: Your hostname, admins-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.4.50 instead (on interface en0)\n",
      "23/11/20 20:41:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/20 20:41:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/20 20:41:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"bank_info\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 20:41:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "path = \"Loan_default.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+----------+-----------+--------------+--------------+------------+--------+--------+-----------+--------------+-------------+-----------+-------------+-----------+-----------+-------+\n",
      "|    LoanID|Age|Income|LoanAmount|CreditScore|MonthsEmployed|NumCreditLines|InterestRate|LoanTerm|DTIRatio|  Education|EmploymentType|MaritalStatus|HasMortgage|HasDependents|LoanPurpose|HasCoSigner|Default|\n",
      "+----------+---+------+----------+-----------+--------------+--------------+------------+--------+--------+-----------+--------------+-------------+-----------+-------------+-----------+-----------+-------+\n",
      "|I38PQUQS96| 56| 85994|     50587|        520|            80|             4|       15.23|      36|    0.44| Bachelor's|     Full-time|     Divorced|        Yes|          Yes|      Other|        Yes|      0|\n",
      "|HPSK72WA7R| 69| 50432|    124440|        458|            15|             1|        4.81|      60|    0.68|   Master's|     Full-time|      Married|         No|           No|      Other|        Yes|      0|\n",
      "|C1OZ6DPJ8Y| 46| 84208|    129188|        451|            26|             3|       21.17|      24|    0.31|   Master's|    Unemployed|     Divorced|        Yes|          Yes|       Auto|         No|      1|\n",
      "|V2KKSFM3UN| 32| 31713|     44799|        743|             0|             3|        7.07|      24|    0.23|High School|     Full-time|      Married|         No|           No|   Business|         No|      0|\n",
      "|EY08JDHTZP| 60| 20437|      9139|        633|             8|             4|        6.51|      48|    0.73| Bachelor's|    Unemployed|     Divorced|         No|          Yes|       Auto|         No|      0|\n",
      "|A9S62RQ7US| 25| 90298|     90448|        720|            18|             2|       22.72|      24|     0.1|High School|    Unemployed|       Single|        Yes|           No|   Business|        Yes|      1|\n",
      "|H8GXPAOS71| 38|111188|    177025|        429|            80|             1|       19.11|      12|    0.16| Bachelor's|    Unemployed|       Single|        Yes|           No|       Home|        Yes|      0|\n",
      "|0HGZQKJ36W| 56|126802|    155511|        531|            67|             4|        8.15|      60|    0.43|        PhD|     Full-time|      Married|         No|           No|       Home|        Yes|      0|\n",
      "|1R0N3LGNRJ| 36| 42053|     92357|        827|            83|             1|       23.94|      48|     0.2| Bachelor's| Self-employed|     Divorced|        Yes|           No|  Education|         No|      1|\n",
      "|CM9L1GTT2P| 40|132784|    228510|        480|           114|             4|        9.09|      48|    0.33|High School| Self-employed|      Married|        Yes|           No|      Other|        Yes|      0|\n",
      "|IA35XVH6ZO| 28|140466|    163781|        652|            94|             2|        9.08|      48|    0.23|High School|    Unemployed|      Married|         No|           No|  Education|         No|      0|\n",
      "|Y8UETC3LSG| 28|149227|    139759|        375|            56|             3|        5.84|      36|     0.8|        PhD|     Full-time|     Divorced|         No|           No|  Education|        Yes|      1|\n",
      "|RM6QSRHIYP| 41| 23265|     63527|        829|            87|             4|        9.73|      60|    0.45|   Master's|     Full-time|     Divorced|        Yes|           No|       Auto|        Yes|      0|\n",
      "|GX5YQOGROM| 53|117550|     95744|        395|           112|             4|        3.58|      24|    0.73|High School|    Unemployed|       Single|         No|           No|       Auto|        Yes|      0|\n",
      "|X0BVPZLDC0| 57|139699|     88143|        635|           112|             4|        5.63|      48|     0.2|   Master's|     Part-time|     Divorced|         No|           No|       Home|         No|      0|\n",
      "|O5DM5MPPNA| 41| 74064|    230883|        432|            31|             2|         5.0|      60|    0.89|   Master's|    Unemployed|      Married|        Yes|           No|       Auto|         No|      0|\n",
      "|ZDDRGVTEXS| 20|119704|     25697|        313|            49|             1|        9.63|      24|    0.28|        PhD|    Unemployed|       Single|        Yes|           No|       Home|         No|      0|\n",
      "|9V0FJW7QPB| 39| 33015|     10889|        811|           106|             2|       13.56|      60|    0.66|   Master's| Self-employed|       Single|        Yes|           No|      Other|         No|      0|\n",
      "|O1IKKLC69B| 19| 40718|     78515|        319|           119|             2|        14.0|      24|    0.17| Bachelor's| Self-employed|     Divorced|        Yes|           No|  Education|         No|      1|\n",
      "|F7487UU2BF| 41|123419|    161146|        376|            65|             4|       16.96|      60|    0.39|High School| Self-employed|       Single|        Yes|           No|      Other|        Yes|      0|\n",
      "+----------+---+------+----------+-----------+--------------+--------------+------------+--------+--------+-----------+--------------+-------------+-----------+-------------+-----------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_df = spark.read.option(\"header\",'True').option('delimiter', ',').csv(path)\n",
    "bank_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 255347\n",
      "Number of columns: 18\n"
     ]
    }
   ],
   "source": [
    "num_rows = bank_df.count()\n",
    "num_columns = bank_df.columns\n",
    "print(\"Number of rows:\", num_rows)\n",
    "print(\"Number of columns:\", len(num_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LoanID: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Income: string (nullable = true)\n",
      " |-- LoanAmount: string (nullable = true)\n",
      " |-- CreditScore: string (nullable = true)\n",
      " |-- MonthsEmployed: string (nullable = true)\n",
      " |-- NumCreditLines: string (nullable = true)\n",
      " |-- InterestRate: string (nullable = true)\n",
      " |-- LoanTerm: string (nullable = true)\n",
      " |-- DTIRatio: string (nullable = true)\n",
      " |-- Education: string (nullable = true)\n",
      " |-- EmploymentType: string (nullable = true)\n",
      " |-- MaritalStatus: string (nullable = true)\n",
      " |-- HasMortgage: string (nullable = true)\n",
      " |-- HasDependents: string (nullable = true)\n",
      " |-- LoanPurpose: string (nullable = true)\n",
      " |-- HasCoSigner: string (nullable = true)\n",
      " |-- Default: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average age of this dataset is 43.5.\n"
     ]
    }
   ],
   "source": [
    "avg_age = bank_df.agg(avg(\"Age\")).first()[0]\n",
    "print(\"The average age of this dataset is {:.1f}.\".format(avg_age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.createOrReplaceTempView(\"bank_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of records in this dataset is: 255347\n",
      "The average Debt to income ratio across the data set is: 0.5002120643673129\n",
      "The average default rate is: 0.11612824901017048, meaning of the 255347 records 29653 defaulted.\n"
     ]
    }
   ],
   "source": [
    "# Calculate average DTIRatio\n",
    "avg_dti_result = spark.sql(\"SELECT AVG(DTIRatio) as avg_dti FROM bank_table\")\n",
    "avg_dti = avg_dti_result.first()[\"avg_dti\"]\n",
    "\n",
    "# Calculate average Default\n",
    "avg_default_result = spark.sql(\"SELECT AVG(Default) as avg_default FROM bank_table\")\n",
    "avg_default = avg_default_result.first()[\"avg_default\"]\n",
    "\n",
    "# Count records with Default=1\n",
    "count_default_df_result = spark.sql(\"SELECT COUNT(*) as count_default FROM bank_table WHERE Default = 1\")\n",
    "count_default_df = count_default_df_result.first()[\"count_default\"]\n",
    "\n",
    "# Count total records\n",
    "bank_count_result = spark.sql(\"SELECT COUNT(*) as bank_count FROM bank_table\")\n",
    "bank_count = bank_count_result.first()[\"bank_count\"]\n",
    "\n",
    "# Display the results\n",
    "print(f\"The total number of records in this dataset is: {bank_count}\")\n",
    "print(f\"The average Debt to income ratio across the data set is: {avg_dti}\")\n",
    "print(f\"The average default rate is: {avg_default}, meaning of the {bank_count} records {count_default_df} defaulted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, LoanID: string, Age: string, Income: string, LoanAmount: string, CreditScore: string, MonthsEmployed: string, NumCreditLines: string, InterestRate: string, LoanTerm: string, DTIRatio: string, Education: string, EmploymentType: string, MaritalStatus: string, HasMortgage: string, HasDependents: string, LoanPurpose: string, HasCoSigner: string, Default: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting PySpark DataFrame to Pandas DataFrame\n",
    "# import pandas as pd\n",
    "# pandas_df = dataframe.toPandas() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
